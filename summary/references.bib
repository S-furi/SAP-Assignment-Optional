@article{sumers2024cognitivearchitectureslanguageagents,
      title={Cognitive Architectures for Language Agents}, 
      author={Theodore R. Sumers and Shunyu Yao and Karthik Narasimhan and Thomas L. Griffiths},
      year={2024},
     eprint={2309.02427},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2309.02427}, 
}
@book{humanproblemsolving,
    author = {Newell, Allen},
    title = {Human Problem Solving},
    year = {1972},
    isbn = {0134454030},
    publisher = {Prentice-Hall, Inc.},
    address = {USA}
}
@article{Post1943-POSFRO,
	author = {Emil L. Post},
	doi = {10.2307/2268006},
	journal = {Journal of Symbolic Logic},
	number = {1},
	pages = {50--52},
	publisher = {Association for Symbolic Logic},
	title = {Formal Reductions of the General Combinatorial Decision Problem},
	volume = {8},
	year = {1943}
}

@article{LANGLEY2009141,
    title = {Cognitive architectures: Research issues and challenges},
    journal = {Cognitive Systems Research},
    volume = {10},
    number = {2},
    pages = {141-160},
    year = {2009},
    issn = {1389-0417},
    doi = {https://doi.org/10.1016/j.cogsys.2006.07.004},
    url = {https://www.sciencedirect.com/science/article/pii/S1389041708000557},
    author = {Pat Langley and John E. Laird and Seth Rogers},
    keywords = {Cognitive architectures, Intelligent systems, Cognitive processes},
    abstract = {In this paper, we examine the motivations for research on cognitive architectures and review some candidates that have been explored in the literature. After this, we consider the capabilities that a cognitive architecture should support, some properties that it should exhibit related to representation, organization, performance, and learning, and some criteria for evaluating such architectures at the systems level. In closing, we discuss some open issues that should drive future research in this important area.}
}

@book{Newell1990-NEWUTO,
	address = {Cambridge},
	author = {Allen Newell},
	editor = {},
	publisher = {Harvard University Press},
	title = {Unified Theories of Cognition},
	year = {1990}
}

@article{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@article{huang2022innermonologueembodiedreasoning,
      title={Inner Monologue: Embodied Reasoning through Planning with Language Models}, 
      author={Wenlong Huang and Fei Xia and Ted Xiao and Harris Chan and Jacky Liang and Pete Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and Pierre Sermanet and Noah Brown and Tomas Jackson and Linda Luu and Sergey Levine and Karol Hausman and Brian Ichter},
      year={2022},
      eprint={2207.05608},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2207.05608}, 
}

@article{yao2023reactsynergizingreasoningacting,
      title={ReAct: Synergizing Reasoning and Acting in Language Models}, 
      author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
      year={2023},
      eprint={2210.03629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.03629}, 
}

@article{alayrac2022flamingovisuallanguagemodel,
      title={Flamingo: a Visual Language Model for Few-Shot Learning}, 
      author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
      year={2022},
      eprint={2204.14198},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.14198}, 
}

@article{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{wang2023selfconsistencyimproveschainthought,
      title={Self-Consistency Improves Chain of Thought Reasoning in Language Models}, 
      author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
      year={2023},
      eprint={2203.11171},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.11171}, 
}

@misc{zhou2023largelanguagemodelshumanlevel,
      title={Large Language Models Are Human-Level Prompt Engineers}, 
      author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},
      year={2023},
      eprint={2211.01910},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.01910}, 
}

@misc{hao2023reasoninglanguagemodelplanning,
      title={Reasoning with Language Model is Planning with World Model}, 
      author={Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and Daisy Zhe Wang and Zhiting Hu},
      year={2023},
      eprint={2305.14992},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14992}, 
}
